{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaEYydG6RkTwJDelSM2U87",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiyaShah2753/ML/blob/main/ML_Lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4zRpZJlg6FD",
        "outputId": "fe2bdda7-1a0b-44f5-cf30-01c8d1846778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from opendatasets) (4.64.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.25.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (8.0.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "# pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install pandas"
      ],
      "metadata": {
        "id": "qM8aCJW1mc3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d359d3e-d2a9-4830-fec4-f22bb6b1c53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from nltk.corpus import stopwords\n",
        "# import string\n",
        "# import re\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from sklearn.metrics import classification_report\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import seaborn as sns\n",
        "# from wordcloud import WordCloud\n",
        "# import matplotlib.pyplot as plt\n",
        "# # columns = [\"sentiment\", \"id\", \"date\", \"query\", \"user_id\", \"text\"]\n",
        "# # df = pd.read_csv(\"sentiment140/training.1600000.processed.noemoticon.csv\", encoding=\"latin\",names=columns)\n",
        "# # df.head()"
      ],
      "metadata": {
        "id": "A5t9dZaEz-eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # df[\"sentiment\"] = df[\"sentiment\"].replace(4,1)\n",
        "# # sns.countplot(x=\"sentiment\",data=df)\n",
        "# # import nltk\n",
        "\n",
        "# # # nltk.download('stopwords')\n",
        "# # positive_tweets = df[df['sentiment'] == 1]['text'].tolist()[6996:7996]\n",
        "# # negative_tweets = df[df['sentiment'] == 0]['text'].tolist()[6996:7996]\n",
        "# # len(positive_tweets)\n",
        "\n",
        "# # Removing hyperlinks and hashtags\n",
        "\n",
        "# # import random\n",
        "\n",
        "# # tweet = positive_tweets[random.randint(0,len(positive_tweets))]\n",
        "# # print('\\033[92m'+tweet)\n",
        "# # print('\\033[93m')\n",
        "\n",
        "# # # remove hyperlinks\n",
        "# # tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',tweet);\n",
        "\n",
        "# # # remove hashtags\n",
        "# # # only removing the hash sign from the word\n",
        "# # tweet2 = re.sub(r'#','',tweet2)\n",
        "\n",
        "# # print(tweet2)\n",
        "# # # Tokenizing the string\n",
        "# # from nltk.tokenize import TweetTokenizer\n",
        "# # tokenizer = TweetTokenizer(preserve_case=False)\n",
        "\n",
        "# # tweet_tokens = tokenizer.tokenize(tweet2)\n",
        "\n",
        "# # print()\n",
        "# # print('Tokenized string: ')\n",
        "# # print(tweet_tokens)\n",
        "\n",
        "# # Removing the stopwords\n",
        "\n",
        "# # stopwords_english = stopwords.words('english')\n",
        "\n",
        "# # print(stopwords_english)\n",
        "\n",
        "# # print(string.punctuation)\n",
        "\n",
        "# # positive_tweets[0]\n",
        "# import re\n",
        "\n",
        "# # Now using all the knowledge, creating clean positive and negative tweets\n",
        "\n",
        "# # Step 1 - Remove hyperlinks and hashtags\n",
        "# for i in range(len(positive_tweets)):\n",
        "#   positive_tweets[i] = re.sub(r'''(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*''','',positive_tweets[i]);\n",
        "#   positive_tweets[i] = re.sub(r'#','',positive_tweets[i])\n",
        "#   negative_tweets[i] = re.sub(r'''(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*''','',negative_tweets[i]);\n",
        "#   negative_tweets[i] = re.sub(r'#','',negative_tweets[i])\n",
        "\n",
        "# # Step 2 - Tokenize the string\n",
        "\n",
        "# tokenizer = TweetTokenizer(preserve_case=False)\n",
        "\n",
        "# for i in range(len(positive_tweets)):\n",
        "#   positive_tweets[i] = tokenizer.tokenize(positive_tweets[i])\n",
        "#   negative_tweets[i] = tokenizer.tokenize(negative_tweets[i])\n",
        "\n",
        "# # Step 3 - Remove the stopwords and punctuations\n",
        "# for i in range(len(positive_tweets)):\n",
        "#   temp_pos = [*positive_tweets[i]]\n",
        "#   positive_tweets[i] = []\n",
        "#   for word in temp_pos:\n",
        "#     if (word not in stopwords_english and word not in string.punctuation):\n",
        "#       positive_tweets[i].append(word)\n",
        "\n",
        "#   temp_neg = [*negative_tweets[i]]\n",
        "#   negative_tweets[i] = []\n",
        "#   for word in temp_neg:\n",
        "#     if (word not in stopwords_english and word not in string.punctuation):\n",
        "#       negative_tweets[i].append(word)\n",
        "#   positive_tweets_storage = positive_tweets\n",
        "# negative_tweets_storage = negative_tweets\n",
        "\n",
        "# positive_tweets[0], negative_tweets[0]"
      ],
      "metadata": {
        "id": "w8Ua3GKz0SCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')\n",
        "# # Lemmatizing all the tweets\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# print(positive_tweets[10][2])\n",
        "\n",
        "# for i in range(len(positive_tweets)):\n",
        "#   for j in range(len(positive_tweets[i])):\n",
        "#     # print(\"here->\",positive_tweets[i][j])\n",
        "#     positive_tweets[i][j] = WordNetLemmatizer().lemmatize(positive_tweets[i][j],'v')\n",
        "\n",
        "# for i in range(len(negative_tweets)):\n",
        "#   for j in range(len(negative_tweets[i])):\n",
        "#     negative_tweets[i][j] = WordNetLemmatizer().lemmatize(negative_tweets[i][j],'v')\n",
        "\n",
        "# positive_tweets[0], negative_tweets[0]\n",
        "\n",
        "# positive_text = []\n",
        "# negative_text = []\n",
        "# for wordlist in positive_tweets:\n",
        "#   positive_text = [*positive_text, \" \".join(wordlist)]\n",
        "\n",
        "# for wordlist in negative_tweets:\n",
        "#   negative_text = [*negative_text, \" \".join(wordlist)]\n",
        "\n",
        "#   from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# vectorizer = CountVectorizer()\n",
        "\n",
        "# # X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# data = []\n",
        "# data_labels = []\n",
        "\n",
        "# for word in positive_text:\n",
        "#   data.append(word)\n",
        "#   data_labels.append(1)\n",
        "\n",
        "# for word in negative_text:\n",
        "#   data.append(word)\n",
        "#   data_labels.append(0)\n",
        "\n",
        "# features = vectorizer.fit_transform(data)\n",
        "# features_nd = features.toarray()\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     features_nd,\n",
        "#     data_labels,\n",
        "#     train_size = 0.85,\n",
        "#     random_state = 81\n",
        "# )\n",
        "\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# model = MultinomialNB()\n",
        "# model.fit(features_nd, data_labels)"
      ],
      "metadata": {
        "id": "D1NTEVk103Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "RwN6h2to1GLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import confusion_matrix\n",
        "# conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "# print(conf_matrix)\n",
        "\n",
        "# from sklearn import metrics\n",
        "\n",
        "# print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "# print(\"Precision:\",metrics.precision_score(y_test,y_pred))\n",
        "# print(\"Recall:\",metrics.recall_score(y_test,y_pred))\n",
        "# print(\"F1 score:\",metrics.f1_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "Gfbq3NqV1HC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "decision tree classifier"
      ],
      "metadata": {
        "id": "kZRSkh1T1PXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn import tree\n",
        "\n",
        "# clf = tree.DecisionTreeClassifier()\n",
        "# clf = clf.fit(X_train, y_train)\n",
        "\n",
        "# y_tree_pred = clf.predict(X_test)\n",
        "\n",
        "# print(\"Accuracy:\",metrics.accuracy_score(y_test, y_tree_pred))\n",
        "# print(\"Precision:\",metrics.precision_score(y_test,y_tree_pred))\n",
        "# print(\"Recall:\",metrics.recall_score(y_test,y_tree_pred))\n",
        "# print(\"F1 score:\",metrics.f1_score(y_test,y_tree_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "DO9lRL3H1QQ9",
        "outputId": "cbc8a3cb-a542-43fb-9441-266f70d76653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fd393e7cf54d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_tree_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# naive_bayes = [metrics.accuracy_score(y_test, y_pred),\n",
        "#      metrics.precision_score(y_test,y_pred),\n",
        "#      metrics.recall_score(y_test,y_pred),\n",
        "#      metrics.f1_score(y_test,y_pred)]\n",
        "\n",
        "# decision_tree = [metrics.accuracy_score(y_test, y_tree_pred),\n",
        "#      metrics.precision_score(y_test,y_tree_pred),\n",
        "#      metrics.recall_score(y_test,y_tree_pred),\n",
        "#      metrics.f1_score(y_test,y_tree_pred)]\n",
        "\n",
        "# df = pd.DataFrame([naive_bayes,decision_tree], index=['Naive Bayes', 'Decision Tree'], columns=['Accuracy','Precision','Recall','F1 Score'])\n",
        "# df\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_tree_pred)\n",
        "# print(conf_matrix)"
      ],
      "metadata": {
        "id": "FwLtVSfV1Wx-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}